{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "730526f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (264621872.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [3]\u001b[0;36m\u001b[0m\n\u001b[0;31m    1st Problem\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "1st Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42029ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the first df 7652\n",
      "the second df1 8466\n",
      "total input 16118\n",
      "DataFrame with Split Columns:\n",
      "+----------+-----------+-------------+----------+-----------+-----+-------------+--------+\n",
      "|first_name|  last_name|date_of_birth|company_id|last_active|score|joined_league|us_state|\n",
      "+----------+-----------+-------------+----------+-----------+-----+-------------+--------+\n",
      "|   Brennan|    Mikayla|   1966/02/11|         2| 07/04/2018|   84|         1989|      IL|\n",
      "|    Holmes|     Thomas|   1962/29/11|         1| 05/15/2018|   92|         1972|      WI|\n",
      "|     Jones|      Corey|   1964/20/12|         7| 08/25/2018|   47|         2007|      NE|\n",
      "|    Howard|      Laura|   1989/26/04|         8| 04/15/2018|   76|         1976|      NE|\n",
      "|Mclaughlin|     Daniel|   1966/19/06|        13| 05/10/2018|   56|         1986|      RH|\n",
      "|   Patrick|    Patrick|   1971/09/08|         7| 06/15/2018|   37|         2013|      VE|\n",
      "|  Browning|     Robert|   1961/02/02|         4| 01/02/2019|   89|         1971|      WI|\n",
      "|   Johnson|        Don|   1962/02/05|         2| 11/17/2018|   56|         1980|      IN|\n",
      "|  Anderson|      Traci|   1987/14/06|         0| 10/21/2018|   33|         1993|      TE|\n",
      "|   Navarro|    Shelley|   1969/11/09|        11| 11/13/2018|   46|         2011|      OH|\n",
      "|  Copeland|      Peggy|   1982/03/08|        17| 01/31/2018|   31|         1991|      CO|\n",
      "|     Lopez|      Kayla|   1997/23/11|        14| 08/20/2018|   62|         2015|      NE|\n",
      "|      Bell|     Sheila|   1988/27/04|        20| 08/25/2018|   59|         2008|      OH|\n",
      "|    Harvey|     Joseph|   1990/12/04|        18| 05/19/2018|   96|         2013|      MA|\n",
      "|    Garcia|      Amber|   1982/25/05|        17| 04/28/2018|   63|         1985|      NE|\n",
      "|  Delacruz|  Christine|   1967/09/01|        17| 12/30/2018|   75|         1997|      AR|\n",
      "|      Pham|        Amy|   1992/18/08|         0| 02/19/2018|   73|         2013|      ID|\n",
      "|   Jackson|   Samantha|   1986/05/11|         4| 04/23/2018|   47|         1978|      WA|\n",
      "|  Williams|    Charles|   1966/27/12|         8| 02/21/2018|   87|         2011|      AL|\n",
      "|   Navarro|Christopher|   1997/10/07|        19| 01/29/2018|   43|         1985|      OH|\n",
      "+----------+-----------+-------------+----------+-----------+-----+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "final_df 16118\n",
      "combined_df\n",
      "+----------+-----------+-------------+----------+-----------+-----+-------------+--------+\n",
      "|first_name|  last_name|date_of_birth|company_id|last_active|score|joined_league|us_state|\n",
      "+----------+-----------+-------------+----------+-----------+-----+-------------+--------+\n",
      "|   Brennan|    Mikayla|   1966/02/11|         2| 07/04/2018|   84|         1989|      IL|\n",
      "|    Holmes|     Thomas|   1962/29/11|         1| 05/15/2018|   92|         1972|      WI|\n",
      "|     Jones|      Corey|   1964/20/12|         7| 08/25/2018|   47|         2007|      NE|\n",
      "|    Howard|      Laura|   1989/26/04|         8| 04/15/2018|   76|         1976|      NE|\n",
      "|Mclaughlin|     Daniel|   1966/19/06|        13| 05/10/2018|   56|         1986|      RH|\n",
      "|   Patrick|    Patrick|   1971/09/08|         7| 06/15/2018|   37|         2013|      VE|\n",
      "|  Browning|     Robert|   1961/02/02|         4| 01/02/2019|   89|         1971|      WI|\n",
      "|   Johnson|        Don|   1962/02/05|         2| 11/17/2018|   56|         1980|      IN|\n",
      "|  Anderson|      Traci|   1987/14/06|         0| 10/21/2018|   33|         1993|      TE|\n",
      "|   Navarro|    Shelley|   1969/11/09|        11| 11/13/2018|   46|         2011|      OH|\n",
      "|  Copeland|      Peggy|   1982/03/08|        17| 01/31/2018|   31|         1991|      CO|\n",
      "|     Lopez|      Kayla|   1997/23/11|        14| 08/20/2018|   62|         2015|      NE|\n",
      "|      Bell|     Sheila|   1988/27/04|        20| 08/25/2018|   59|         2008|      OH|\n",
      "|    Harvey|     Joseph|   1990/12/04|        18| 05/19/2018|   96|         2013|      MA|\n",
      "|    Garcia|      Amber|   1982/25/05|        17| 04/28/2018|   63|         1985|      NE|\n",
      "|  Delacruz|  Christine|   1967/09/01|        17| 12/30/2018|   75|         1997|      AR|\n",
      "|      Pham|        Amy|   1992/18/08|         0| 02/19/2018|   73|         2013|      ID|\n",
      "|   Jackson|   Samantha|   1986/05/11|         4| 04/23/2018|   47|         1978|      WA|\n",
      "|  Williams|    Charles|   1966/27/12|         8| 02/21/2018|   87|         2011|      AL|\n",
      "|   Navarro|Christopher|   1997/10/07|        19| 01/29/2018|   43|         1985|      OH|\n",
      "+----------+-----------+-------------+----------+-----------+-----+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ReadTabSeparatedData\").getOrCreate()\n",
    "\n",
    "# Specify Input file path\n",
    "file_path1 = \"./us_softball_league.tsv\"\n",
    "file_path2 = \"./unity_golf_club.csv\"\n",
    "\n",
    "# Read tab-separated and comma separted data using Spark\n",
    "df = spark.read.csv(file_path1, sep='\\t', header=True, inferSchema=True)\n",
    "df1 = spark.read.csv(file_path2, sep=',', header=True, inferSchema=True)\n",
    "\n",
    "print(\"the first df\",df.count())\n",
    "print(\"the second df1\",df1.count())\n",
    "print(\"total input\",df.count()+df1.count())\n",
    "# df1.show()\n",
    "# Display the DataFrame\n",
    "#df.show()\n",
    "#df.printSchema()\n",
    "df_split = df.withColumn(\"first_name\", split(col(\"name\"), \" \")[1])\\\n",
    "             .withColumn(\"last_name\", split(col(\"name\"), \" \")[0]) \\\n",
    "             .withColumn('date_of_birth',date_format(to_date(col(\"date_of_birth\"),\"MM/dd/yyyy\"),'yyyy/dd/MM'))\\\n",
    "             .select(\"first_name\", \"last_name\", *df.columns) \\\n",
    "             .withColumn(\"us_state\", upper(substring(col(\"us_state\"), 1, 2)))\\\n",
    "             .drop(\"name\")\n",
    "\n",
    "# Display the DataFrame with split columns\n",
    "print(\"DataFrame with Split Columns:\")\n",
    "df_split.show()\n",
    "\n",
    "\n",
    "combined_df = df_split.union(df1)\n",
    "print(\"final_df\",combined_df.count())\n",
    "print(\"combined_df\")\n",
    "combined_df.show()\n",
    "\n",
    "# Save the combined DataFrame to a master file\n",
    "output_file_name = \"./final_file\"\n",
    "df.write.csv(output_file_name, header=True, mode=\"overwrite\")\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86154ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Initialize Spark session\n",
    "#spark = SparkSession.builder.appName(\"DataModelExample\").getOrCreate()\n",
    "\n",
    "df_large_file = spark.read.csv(file_path1, sep=',', header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "\n",
    "# Write data to a local PostgreSQL database\n",
    "db_url = \"jdbc:postgresql://localhost:5432/your_database_name\"\n",
    "properties = {\"user\": \"your_username\", \"password\": \"your_password\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "df_data.write.jdbc(url=db_url, table=\"table_data, mode=\"overwrite\", properties=properties)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11187f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+-----------+-----+-------------+------------+--------------------+\n",
      "|               name|date_of_birth|last_active|score|joined_league|    us_state|          company_id|\n",
      "+-------------------+-------------+-----------+-----+-------------+------------+--------------------+\n",
      "|    Mikayla Brennan|   11/02/1966| 07/04/2018|   84|         1989|    Illinois|        Keller Group|\n",
      "|      Thomas Holmes|   11/29/1962| 05/15/2018|   92|         1972|   Wisconsin|Brown, Vasquez an...|\n",
      "|        Corey Jones|   12/20/1964| 08/25/2018|   47|         2007|  New Mexico|Alvarez, Schaefer...|\n",
      "|       Laura Howard|   04/26/1989| 04/15/2018|   76|         1976|  New Jersey|Smith, Torres and...|\n",
      "|  Daniel Mclaughlin|   06/19/1966| 05/10/2018|   56|         1986|Rhode Island|Mullen, Huffman a...|\n",
      "|    Patrick Patrick|   08/09/1971| 06/15/2018|   37|         2013|     Vermont|Alvarez, Schaefer...|\n",
      "|    Robert Browning|   02/02/1961| 01/02/2019|   89|         1971|   Wisconsin|           Bruce Inc|\n",
      "|        Don Johnson|   05/02/1962| 11/17/2018|   56|         1980|     Indiana|        Keller Group|\n",
      "|     Traci Anderson|   06/14/1987| 10/21/2018|   33|         1993|   Tennessee| Williams-Stephenson|\n",
      "|    Shelley Navarro|   09/11/1969| 11/13/2018|   46|         2011|        Ohio|       Rivera-Morrow|\n",
      "|     Peggy Copeland|   08/03/1982| 01/31/2018|   31|         1991| Connecticut|   Peterson and Sons|\n",
      "|        Kayla Lopez|   11/23/1997| 08/20/2018|   62|         2015|  New Mexico|Jackson, Carlson ...|\n",
      "|        Sheila Bell|   04/27/1988| 08/25/2018|   59|         2008|        Ohio|                null|\n",
      "|      Joseph Harvey|   04/12/1990| 05/19/2018|   96|         2013|       Maine|Hopkins, Barnes a...|\n",
      "|       Amber Garcia|   05/25/1982| 04/28/2018|   63|         1985|    New York|   Peterson and Sons|\n",
      "| Christine Delacruz|   01/09/1967| 12/30/2018|   75|         1997|     Arizona|   Peterson and Sons|\n",
      "|        Amy Pham MD|   08/18/1992| 02/19/2018|   73|         2013|       Idaho| Williams-Stephenson|\n",
      "|   Samantha Jackson|   11/05/1986| 04/23/2018|   47|         1978|  Washington|           Bruce Inc|\n",
      "|   Charles Williams|   12/27/1966| 02/21/2018|   87|         2011|     Alabama|Smith, Torres and...|\n",
      "|Christopher Navarro|   07/10/1997| 01/29/2018|   43|         1985|        Ohio|          Rivera Ltd|\n",
      "+-------------------+-------------+-----------+-----+-------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast, col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"BroadcastJoinExample\").getOrCreate()\n",
    "\n",
    "# Sample DataFrames\n",
    "file_path1 = \"./us_softball_league.tsv\"\n",
    "file_path2 = \"./companies.csv\"\n",
    "\n",
    "# Read tab-separated and comma-separated data using Spark\n",
    "df_large_file = spark.read.csv(file_path1, sep='\\t', header=True, inferSchema=True)\n",
    "df_small_file = spark.read.csv(file_path2, sep=',', header=True, inferSchema=True)\n",
    "\n",
    "# Show the small and large DataFrames\n",
    "# df_small_file.show()\n",
    "# df_large_file.show()\n",
    "\n",
    "\n",
    "\n",
    "# Broadcast the small DataFrame\n",
    "broadcast_mapping_state = broadcast(df_small_file)\n",
    "\n",
    "# Register the small DataFrame as a temporary SQL table\n",
    "broadcast_mapping_state.createOrReplaceTempView(\"company_mapping\")\n",
    "\n",
    "# Register the large DataFrame as a temporary SQL table\n",
    "df_large_file.createOrReplaceTempView(\"large_table\")\n",
    "\n",
    "# Define a Spark SQL query to replace company_id with company name\n",
    "sql_query = \"\"\"\n",
    "    SELECT l.name,l.date_of_birth,l.last_active,l.score,l.joined_league,l.us_state, c.name AS company_id\n",
    "    FROM large_table l\n",
    "    LEFT JOIN company_mapping c ON l.company_id = c.id\n",
    "\"\"\"\n",
    "\n",
    "# Execute the Spark SQL query\n",
    "df_output = spark.sql(sql_query)\n",
    "\n",
    "# Show the result\n",
    "df_output.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4c37be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "# Create a Spark session\n",
    "#spark = SparkSession.builder.appName(\"DataCleaningExample\").getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "file_path = \"./us_softball_league.tsv\"\n",
    "df = spark.read.csv(file_path, sep='\\t', header=True, inferSchema=True)\n",
    "\n",
    "# Show the original DataFrame\n",
    "df.show()\n",
    "\n",
    "# Define the criteria for impossible chronological combinations\n",
    "def is_bad_record(date_of_birth, last_active):\n",
    "    # Add your specific criteria for impossible chronological combinations\n",
    "    return to_date(date_of_birth, 'MM/dd/yyyy') > to_date(last_active, 'MM/dd/yyyy')\n",
    "\n",
    "# Register the UDF\n",
    "is_bad_record_udf = spark.udf.register(\"is_suspect_record\", is_suspect_record, BooleanType())\n",
    "\n",
    "# Filter suspect records and write them to a separate file\n",
    "suspect_records = df.filter(is_bad_record_udf(col(\"date_of_birth\"), col(\"last_active\")))\n",
    "suspect_records.write.mode(\"overwrite\").csv(\"./suspect_records.csv\", header=True, sep='\\t')\n",
    "\n",
    "# Drop suspect records from the main DataFrame\n",
    "cleaned_df = df.filter(~is_suspect_record_udf(col(\"date_of_birth\"), col(\"last_active\")))\n",
    "\n",
    "# Show the cleaned DataFrame\n",
    "cleaned_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3e3447",
   "metadata": {},
   "outputs": [],
   "source": [
    "Part 3: Follow-up\n",
    "    \n",
    "Readme.md\n",
    "1.Remove any redundant or unnecessary code breaking it into modularity.\n",
    "2.Use some Environemtn kind of varibale.\n",
    "3.Perform Unit testing\n",
    "4.Making the diffrenent Depolyment Environment\n",
    "5.Use some orchestartion tool\n",
    "6.Enhance code documentation for better understanding.\n",
    "7.USE DRY PRINCIPLE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
